<h1 align="center">AlpaGasus: Training a Better Alpaca with Fewer Data (ICLR 2024)</h1>
<h4 align="center"> Lichang Chen*, Shiyang Li*, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin</h4>
<h4 align="center"> *Denotes equal contribution</h4>

## [Project page](https://lichang-chen.github.io/AlpaGasus/) | [Paper](https://arxiv.org/abs/2307.08701)


<p align="center">
    <img src="alpagasus.jpeg" width="50%"> <br>
    Our Model "AlpaGasus"is pronounced as "/ˈælpəˈɡeɪsəs/", or "/ˈælpəˈɡəsəs/". The logo is generated by <a href="https://www.midjourney.com/app/">Midjourney</a>
</p>

## News
- [2023.7] We release our paper. If you have any questions about our project, please send email to bobchen@umd.edu
- [2023.9] Thanks to @[GPT4animal](https://github.com/gpt4life/alpagasus) for reimplementing the results in our paper. Please check this fantastic repo: https://github.com/gpt4life/alpagasus.
- [2023.9] Thanks to @[gauss5930](https://github.com/gauss5930) and @[YooYunS](https://github.com/YooYunS) who implemented the QLoRA version of Alpagasus-7B and 13B, which could be run on the customer-level GPUs. please refer to their repo: [Alpagasus2-QLoRA](https://github.com/gauss5930/AlpaGasus2-QLoRA) They also show that tuning LLaMA-2 could achieve better performance.



## Citation
If you find our paper useful, please consider citing:
```bibtex
@article{chen2023alpagasus,
  title={AlpaGasus: Training a Better Alpaca with Fewer Data},
  author={Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, Hongxia Jin},
  journal={arXiv preprint arXiv:2307.08701},
  year={2023}
}
```
